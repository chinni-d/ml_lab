{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Define the dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y = np.array([[0], [1], [1], [0]], dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the architecture of the neural network\n",
    "n_input = 2\n",
    "n_hidden = 2\n",
    "n_output = 1\n",
    "\n",
    "# Define the weights and biases as TensorFlow variables\n",
    "weights = {\n",
    "    'hidden': tf.Variable(tf.random.normal([n_input, n_hidden])),\n",
    "    'output': tf.Variable(tf.random.normal([n_hidden, n_output]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'hidden': tf.Variable(tf.random.normal([n_hidden])),\n",
    "    'output': tf.Variable(tf.random.normal([n_output]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the forward pass\n",
    "def forward_propagation(x):\n",
    "    hidden_layer = tf.sigmoid(tf.add(tf.matmul(x, weights['hidden']), biases['hidden']))\n",
    "    output_layer = tf.sigmoid(tf.add(tf.matmul(hidden_layer, weights['output']), biases['output']))\n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the backpropagation algorithm\n",
    "def backpropagation(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        output_layer = forward_propagation(x)\n",
    "        loss = tf.reduce_mean(0.5 * (y - output_layer) ** 2)\n",
    "    \n",
    "    gradients = tape.gradient(loss, [weights['hidden'], weights['output'], biases['hidden'], biases['output']])\n",
    "    optimizer.apply_gradients(zip(gradients, [weights['hidden'], weights['output'], biases['hidden'], biases['output']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.16701394319534302\n",
      "Epoch: 1000, Loss: 0.11679938435554504\n",
      "Epoch: 2000, Loss: 0.10932266712188721\n",
      "Epoch: 3000, Loss: 0.09658908098936081\n",
      "Epoch: 4000, Loss: 0.0756923258304596\n",
      "Epoch: 5000, Loss: 0.05234337970614433\n",
      "Epoch: 6000, Loss: 0.03388703614473343\n",
      "Epoch: 7000, Loss: 0.022483371198177338\n",
      "Epoch: 8000, Loss: 0.01583809219300747\n",
      "Epoch: 9000, Loss: 0.011820018291473389\n"
     ]
    }
   ],
   "source": [
    "# Define the training loop\n",
    "optimizer = tf.optimizers.SGD(learning_rate=0.1)\n",
    "epochs = 10000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    backpropagation(X, y)\n",
    "    if epoch % 1000 == 0:\n",
    "        output = forward_propagation(X)\n",
    "        loss = tf.reduce_mean(0.5 * (y - output) ** 2)\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# Test the trained model\n",
    "predictions = forward_propagation(X)\n",
    "print(\"Predictions:\")\n",
    "print(predictions.numpy().round())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
